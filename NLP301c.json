{
  "quizSrc": [
    {
        "term": "When performing logistic regression on sentiment analysis, you represented each tweet as a vector of ones and zeros. However your model did not work well. Your training cost was reasonable, but your testing cost was just not acceptable. What could be a possible reason? A. The vector representations are sparse and therefore it is much harder for your model to learn anything that could generalize well to the test set. B. You probably need to increase your vocabulary size because it seems like you have very little features. C. Logistic regression does not work for sentiment analysis, and therefore you should be looking at other models. D. Sparse representations require a good amount of training time so you should train your model for longer",
        "definition": "A. The vector representations are sparse and therefore it is much harder for your model to learn anything that could generalize well to the test set."
    }, {
        "term": "Which of the following are examples of text preprocessing? A. Stemming, or the process of reducing a word to its word stem. B. Lowercasing, which is the process of removing changing all capital letter to lower case. C. Removing stopwords, punctuation, handles and URLs D. Adding new words to make sure all the sentences make sense",
        "definition": "A. Stemming, or the process of reducing a word to its word stem. B. Lowercasing, which is the process of removing changing all capital letter to lower case. C. Removing stopwords, punctuation, handles and URLsBC"
    }, {
        "term": "The sigmoid function is defined as . Which of the following is true. | Large positive values of will make closer to 1 and large negative values of will make close to -1. | Large positive values of will make closer to 1 and large negative values of will make close to 0. | Small positive values of will make closer to 1 and large positive values of will make close to 0. | Small positive values of will make closer to 0 and large negative values of will make close to -1.",
        "definition": "Large positive values of will make closer to 1 and large negative values of will make close to 0."
    }, {
        "term": "The cost function for logistic regression is defined as . Which of the following is true about the cost function above. Mark all the correct ones. |When , as goes close to 0, the cost function approaches . | When , as goes close to 0, the cost function approaches . | When , as goes close to 0, the cost function approaches . | When , as goes close to 0, the cost function approaches .",
        "definition": "When , as goes close to 0, the cost function approaches . | When , as goes close to 0, the cost function approaches ."
    },
    {
        "term": "For what value of in the sigmoid function does .",
        "definition": "0"
    },
    {
        "term": "Quiz type: checkbox, Select all that apply. When performing logistic regression for sentiment analysis using the method taught in this week's lecture, you have to: | Perform data processing. | Create a dictionary that maps the word and the class that word is found in to the number of times that word is found in the class. | Create a dictionary that maps the word and the class that word is found in to see if that word shows up in the class. | For each tweet, you have to create a  positive feature with the sum of positive counts of each word in that tweet. You also have to create a negative feature with the sum of negative counts of each word in that tweet.",
        "definition": "Perform data processing.|Create a dictionary that maps the word and the class that word is found in to the number of times that word is found in the class.|For each tweet, you have to create a  positive feature with the sum of positive counts of each word in that tweet. You also have to create a negative feature with the sum of negative counts of each word in that tweet."
    },
    {
        "term": "When training logistic regression, you have to perform the following operations in the desired order. | Initialize parameters, get gradient, classify/predict, update, get loss, repeat | Initialize parameters, classify/predict, get gradient, update, get loss, repeat | Initialize parameters, get gradient, update, classify/predict, get loss, repeat | Initialize parameters, get gradient, update, get loss, classify/predict, repeat",
        "definition": "Initialize parameters, classify/predict, get gradient, update, get loss, repeat"
    },
    {
        "term": "Assuming we got the classification correct, where for some specific example i. This means that . Which of the following has to hold:| Our prediction, for this specific training example is exactly equal to its corresponding label .| Our prediction, for this specific training example is less than ( ).| Our prediction, for this specific training example is less than .| Our prediction, for this specific training example is greater than .",
        "definition": "Our prediction, for this specific training example is greater than ."
    },
    {
        "term": "What is the purpose of gradient descent? Select all that apply.|Gradient descent allows us to learn the parameters in logistic regression as to minimize the loss function J.| Gradient descent allows us to learn the parameters in logistic regression as to maximize the loss function J.| Gradient descent, grad_theta allows us to update the parameters by computing | Gradient descent, grad_theta allows us to update the parameters by computing ",
        "definition": "Gradient descent allows us to learn the parameters in logistic regression as to minimize the loss function J. | Gradient descent, grad_theta allows us to update the parameters by computing "
    },
    {
        "term": "What is a good metric that allows you to decide when to stop training/trying to get a good model? Select all that apply. | When your accuracy is good enough on the test set. | When your accuracy is good enough on the train set. | When you plot the cost versus (# of iterations) and you see that your the loss is converging (i.e. no longer changes as much). | When \nùõº\nŒ±\nalpha\n, your step size is neither too small nor too large.",
        "definition": "When your accuracy is good enough on the test set.|When you plot the cost versus (# of iterations) and you see that your the loss is converging (i.e. no longer changes as much)."
    }, {
        "term": "Assume that there are 2 happy people and 2 unhappy people in a room. Concretely, persons A and B are happy and persons C and D are unhappy. If you were to randomly pick a person from the room, what is the probability that the person is happy. | 1/2 | 1/4 | 3/4 | 0",
        "definition": "1/2 "
    }, {
        "term": "Assume that there are 2 happy people and 2 unhappy people in a room. Concretely, persons A and B are happy and persons C and D are unhappy. If a friend showed you the part of the room where the two happy people are, what is the probability that you choose person B? | A.1/2 | B.1/4 | C.3/4 |  D.1 ",
        "definition": " D.1 "
    }, {
        "term": "From the equations presented below, express the probability of a tweet being positive given that it contains the word happy in terms of the probability of a tweet containing the word happy given that it is positive | a | b | c | d  ",
        "definition": " a "
    }, {
        "term": "Bayes rule is defined as | a | b | c | d",
        "definition": "a"
    }, {
        "term": "Suppose that in your dataset, 25% of the positive tweets contain the word ‚Äòhappy‚Äô. You also know that a total of 13% of the tweets in your dataset contain the word 'happy', and that 40% of the total number of tweets are positive. You observe the tweet: ''happy to learn NLP'. What is the probability that this tweet is positive? (Please, round your answer up to two decimal places. Remember that 0.578 = 0.58 and 0.572 = 0.57)",
        "definition": "0.77"
    }, {
        "term": "The log likelihood for a certain word is defined as: . | Positive numbers imply that the word is positive. | Positive numbers imply that the word is negative.| Negative numbers imply that the word is negative. | Negative numbers imply that the word is positive.",
        "definition": "Positive numbers imply that the word is positive. | Negative numbers imply that the word is negative."
    }, {
        "term": "The log likelihood mentioned in lecture, which is the log of the ratio between two probabilities is bounded between | -1 and 1 |and| 0 and |0 and 1 | 1 point",
        "definition": "and"
    }, {
        "term": "When implementing naive Bayes, in which order should the following steps be implemented. A. Get or annotate a dataset with positive and negative tweets Preprocess the tweets: process_tweet(tweet) ‚ûû Compute freq(w, class) Get P(w | pos), P(w | neg) Get Œª(w) Compute logprior = log(P(pos) / P(neg)) B. Get or annotate a dataset with positive and negative tweets Preprocess the tweets: process_tweet(tweet) ‚ûû Compute freq(w, class) Get Œª(w) Get P(w | pos), P(w | neg) Compute logprior = log(P(pos) / P(neg)) C. Get or annotate a dataset with positive and negative tweets Compute freq(w, class) Preprocess the tweets: process_tweet(tweet) ‚ûû Get P(w | pos), P(w | neg) Get Œª(w) Compute logprior = log(P(pos) / P(neg)) D. Get or annotate a dataset with positive and negative tweets Compute freq(w, class) Preprocess the tweets: process_tweet(tweet) ‚ûû Compute logprior = log(P(pos) / P(neg) Get P(w | pos), P(w | neg) Get Œª(w)",
        "definition": "A. Get or annotate a dataset with positive and negative tweets Preprocess the tweets: process_tweet(tweet) ‚ûû Compute freq(w, class) Get P(w | pos), P(w | neg) Get Œª(w) Compute logprior = log(P(pos) / P(neg))"
    }, {
        "term": "To test naive bayes model, which of the following are required? | a | b | c |d",
        "definition": "a"
    }, {
        "term": "Which of the following is NOT an application of naive Bayes? A. Sentiment Analysis B. Author identification C. Information retrieval D. Word disambiguation E. Numerical predictions",
        "definition": "E. Numerical predictions"
    }, {
        "term": "Given a corpus A, encoded as and corpus B encoded as , What is the euclidean distance between the two documents? | 5.91608 | 35 | 2.43 | None of the above",
        "definition": "5.91608"
    }, {
        "term": "Given the previous problem, a user now came up with a corpus C defined as and you want to recommend a document that is similar to it. Would you recommend document A or document B? | Document A | Document B‚Äã",
        "definition": "Document A"
    }, {
        "term": "Which of the following is true about euclidean distance? | When comparing similarity between two corpuses, it does not work well when the documents are of different sizes. | It is the norm of the difference between two vectors. | It is a method that makes use of the angle between two vectors | It is the norm squared of the difference between two vectors.",
        "definition": "When comparing similarity between two corpuses, it does not work well when the documents are of different sizes. | It is the norm of the difference between two vectors."
    }, {
        "term": "What is the range of a cosine similarity score, namely s, in the case of information retrieval where the vectors are positive? | a | b | c | d",
        "definition": "c"
    }, {
        "term": "The cosine similarity score of corpus A = and corpus B = is equal to ?| 0.08512565307587486| 0 | 1.251903 | -0.3418283",
        "definition": "0.08512565307587486"
    }, {
        "term": "We will define the following vectors, USA = , Washington = , Turkey = , Ankara = , Russia = , and Japan = . Using only the following vectors, Ankara is the capital of what country? Please consider the cosine similarity score in your calculations.| Japan| Russia| Morocco| Turkey",
        "definition": "Turkey"
    }, {
        "term": "Please select all that apply. PCA is| used to reduce the dimension of your data;| visualize word vectors;| make predictions;| label data.",
        "definition": "used to reduce the dimension of your data;|visualize word vectors;"
    }, {
        "term": "Please select all that apply. Which is correct about PCA?| You can think of an eigenvector as an uncorrelated feature for your data.| The eigenvalues tell you the amount of information retained by each feature.| If working with features in different scales, you do not have to mean normalize.| Computing the covariance matrix is critical when performing PCA",
        "definition": "You can think of an eigenvector as an uncorrelated feature for your data.| The eigenvalues tell you the amount of information retained by each feature.|Computing the covariance matrix is critical when performing PCA"
    }, {
        "term": "I‚Äãn which order do you perform the following operations when computing PCA? A. m‚Äãean normalize, get Œ£ the covariance matrix, perform SVD, then dot product the data, namely X, with a subset of the columns of U to get the reconstruction of your data. B. m‚Äãean normalize, perform SVD, get Œ£ the covariance matrix, then dot product the data, namely X, with a subset of the columns of U to get the reconstruction of your data. C. get Œ£ the covariance matrix, perform SVD, then dot product the data, namely X, with a subset of the columns of U to get the reconstruction of your data, m‚Äãean normalize. D. get Œ£ the covariance matrix, m‚Äãean normalize, perform SVD, then dot product the data, namely X, with a subset of the columns of U to get the reconstruction of your dataIn which order do you perform the following operations when computing PCA?| mean normalize, get the covariance matrix, perform SVD, then dot product the data, namely X, with a subset of the columns of U to get the reconstruction of your data.| mean normalize, perform SVD, get the covariance matrix, then dot product the data, namely X, with a subset of the columns of U to get the reconstruction of your data.| get the covariance matrix, perform SVD, then dot product the data, namely X, with a subset of the columns of U to get the reconstruction of your data, mean normalize.| get the covariance matrix, mean normalize, perform SVD, then dot product the data, namely X, with a subset of the columns of U to get the reconstruction of your data.",
        "definition": "mean normalize, get the covariance matrix, perform SVD, then dot product the data, namely X, with a subset of the columns of U to get the reconstruction of your data."
    }, {
        "term": "Vector space models allow us to | To represent words and documents as vectors. | build useful applications including and not limited to, information extraction, machine translation, and chatbots. | create representations that capture similar meaning. | build faster training algorithms",
        "definition": "To represent words and documents as vectors. | build useful applications including and not limited to, information extraction, machine translation, and chatbots. | create representations that capture similar meaning."
    }, {
        "term": "Assume that your objective is to minimize the transformation of X as similar to Y as possible, what would you optimize to get R? ) | Minimize the distance between XR and Y | Maximize the distance between XR and Y | Minimize the dot product between XR and Y | Maximize the dot product between XR and Y",
        "definition": "Minimize the distance between XR and Y"
    }, {
        "term": "When solving for , which of the following is true? | Create a forloop, inside the forloop: (initialize R, compute the gradient, update the loss | Create a forloop, inside the forloop: (initialize R, update the loss, compute the gradient. | Initialize R, create a forloop, inside the forloop: (compute the gradient, update the loss) | Initialize R, compute the gradient, create a forloop, inside the forloop: (update the loss)",
        "definition": "Initialize R, create a forloop, inside the forloop: (compute the gradient, update the loss)"
    }, {
        "term": "The Frobenius norm of A = is (Answer should be in 2 decimal places)",
        "definition": "7.14"
    }, {
        "term": "Assume which of the following is the gradient of ? | a | b | c | d",
        "definition": "a"
    }, {
        "term": "Imagine that you are visiting a city in the US. If you search for friends that are living in the US, would you be able to determine the 2 closest of ALL your friends around the world? | Yes, because I am already in the country and that implies that my closest friends are also going to be in the same country. | No",
        "definition": "No"
    }, {
        "term": "What is the purpose of using a function to hash vectors into values? | To speed up the time it takes when comparing similar vectors. | To not have to spend time comparing vectors with other vectors that are completely different. | To make the search for other similar vectors more accurate. | It helps us create vectors.",
        "definition": "To speed up the time it takes when comparing similar vectors. | To not have to spend time comparing vectors with other vectors that are completely different."
    }, {
        "term": "Given the following vectors, determine the true statements. : : : : | and have the same sign. | and are equal in magnitude. | and have the same sign.",
        "definition": "and have the same sign."
    }, {
        "term": "We define H to be the number of planes and to be 1 or 0 depending on the sign of the dot product with plane i. Which of the following is the equation used to calculate the hash for several planes. | a | b | c | d",
        "definition": "a"
    }, {
        "term": "How can you speed up the look up for similar documents. | PCA | Approximate Nearest Neighbors | K-Means | Locality sensitive hashing",
        "definition": "Approximate Nearest Neighbors|Locality sensitive hashing"
    }, {
        "term": "Hash tables are useful because | allow us to divide vector space to regions. | speed up look up | classify with higher accuracy | can always be reproduced",
        "definition": "allow us to divide vector space to regions.|speed up look up|can always be reproduced"
    }, {
        "term": "Quiz type: number, The minimum edit distance between the words deep and creepy is: | ",
        "definition": "4"
    },{
        "term": "Quiz type: radio, Which of the following is a NOT VALID example of an edit string operation? | INSERT a letter: ‚Äòaple‚Äô --> ‚Äòapple‚Äô | DELETE a letter: ‚Äòcloack‚Äô --> ‚Äòcloak‚Äô | SWITCH a letter ‚ÄòLusca‚Äô --> ‚ÄòLucas‚Äô | REPLACE a letter ‚ÄòCrayom‚Äô --> ‚ÄòCrayon‚Äô",
        "definition": "SWITCH a letter ‚ÄòLusca‚Äô --> ‚ÄòLucas‚Äô"
    },
    {
        "term": "Quiz type: radio, Autocorrect is only appliable when dealing with misspelled words. | False | True",
        "definition": "False"
    },
    {
        "term": "Quiz type: radio, Given the corpus: ‚ÄúI am happy because I am doing quizzes.‚Äù Based on this tiny corpus, consider the following sentence: ‚ÄúI sm very good at solving quizzes.‚Äù Which of the following is true? | It is not possible to decide a correction for the misspelled word ‚Äúsm‚Äù. | There is a unique correction for the misspelled word ‚Äúsm‚Äù. | There is more than one possible candidate for a correction to the misspelled word ‚Äúsm‚Äù. | The corpus is too tiny, so it is not possible to build a probabilistic model for autocorrection.",
        "definition": "There is a unique correction for the misspelled word ‚Äúsm‚Äù."
    },
    {
        "term": "Quiz type: checkbox, About the probabilistic model defined in the lecture, select all that apply. | Words with the same probability in the corpus will be equally likely to be candidates for a possible word correction. | Replacing a character costs more than deleting a character. | If is the number of times a word appear in a corpus and is the corpus size, then the probability of the word in the corpus is . | The sentence ‚ÄúHappy birthday deer friends‚Äù would not have any word corrected in the model defined in the lecture.",
        "definition": "Replacing a character costs more than deleting a character. | If is the number of times a word appear in a corpus and is the corpus size, then the probability of the word in the corpus is . | The sentence ‚ÄúHappy birthday deer friends‚Äù would not have any word corrected in the model defined in the lecture."
    },
    {
        "term": "Quiz type: number, Suppose we build a distance matrix D for the following case: Source: Pie --> Target: Bye What is the value for D[3,2]? | ",
        "definition": "5"
    },
    {
        "term": "Quiz type: checkbox, About the Minimum edit distance algorithm, select all that apply. Let be the distance matrix, for two words of same size. The matrix size is . | if . | stores the highest value in the matrix. | a | The algorithm avoids usage of brute force by implementing a dynamic programming approach.",
        "definition": "if .|The algorithm avoids usage of brute force by implementing a dynamic programming approach."
    },
    {
        "term": "Quiz type: radio, About the minimum edit distance, which of the following statement is not true? | It is used to evaluate similarity between two strings. | It is used to check if a word is misspelled. | It counts the minimum number of edits to transform one string into another. | It is used to implement spelling correction, document similarity and machine translation.",
        "definition": "It is used to check if a word is misspelled."
    },
    {
        "term": "Quiz type: radio, The minimum edit distance calculation is more computationally intensive if we have a big corpus. | True | False",
        "definition": "False"
    },
    {
        "term": "Quiz type: number, Given the corpus ‚ÄúAutocorrect is a powerful tool and it is used on our computer.‚Äù The value for is: The answer should have two decimal places (rounding up, if necessary). For example: 0.88888 should be answered as 0.89. | ",
        "definition": "0.17"
    },
    {
        "term": "Quiz type: radio, The Transition matrix A defined in lecture allows you to: | Compute the probability of going from a part of speech tag to another part of speech tag.  | Compute the probability of going from a word to a part of speech tag.  | Compute the probability of going from a word to another word.  | Compute the probability of going from a part of speech tag to a word. ",
        "definition": "Compute the probability of going from a part of speech tag to another part of speech tag."
    },
    {
        "term": "Quiz type: radio, The Emission matrix B defined in lecture allows you to: | Compute the probability of going from a word to another word.  | Compute the probability of going from a part of speech tag to another part of speech tag.  | Compute the probability of going from a part of speech tag to a word.  | Compute the probability of going from a word to a part of speech tag. ",
        "definition": "Compute the probability of going from a part of speech tag to a word."
    },
    {
        "term": "Quiz type: radio, The column sum of the emission matrix has to be equal to 1.  | True. | False.",
        "definition": "False."
    },
    {
        "term": "Quiz type: radio, The row sum of the transition matrix has to be 1.  | False, it has to be the column sum. | True",
        "definition": "True"
    },
    {
        "term": "Quiz type: checkbox, Why is smoothing usually applied? Select all that apply. | Applying smoothing, for the majority of cases, allows us to decrease the probabilities in the transition and emission matrices and this allows us to have non zero probabilities.  | Applying smoothing is a bad idea and we should not use it.  | Applying smoothing, for the majority of cases, allows us to increase the probabilities in the transition and emission matrices and this allows us to have non zero probabilities.  | Applying smoothing, for the minority of cases, allows us to increase the probabilities in the transition and emission matrices and this allows us to have non zero probabilities. ",
        "definition": "Applying smoothing, for the majority of cases, allows us to decrease the probabilities in the transition and emission matrices and this allows us to have non zero probabilities.|Applying smoothing, for the majority of cases, allows us to increase the probabilities in the transition and emission matrices and this allows us to have non zero probabilities."
    },
    {
        "term": "Quiz type: radio, Given the following D matrix, what would be the sequence of tags for the words on the right?  | t2,t3,t1,t3,t1  | t3,t4,t2,t3,t1  | t3,t4,t2,t2,t1  | t1,t3,t1,t2,t1 ",
        "definition": "t2,t3,t1,t3,t1"
    },
    {
        "term": "Quiz type: radio, Previously, we have been multiplying the raw probabilities, but in reality we take the log of those probabilities. Why might that be the case? | We take the log probabilities because probabilities are bounded between 0 and 1 and as a result, the numbers could be too small and will go towards 0.  | The log probabilities should not be used because they introduce noise to our original computed scores.  | The log probabilities help us with the inference as they bound the numbers between -1 and 1.  | Because the log probabilities force the numbers to be between 0 and 1 and hence, we want to take a probability.",
        "definition": "We take the log probabilities because probabilities are bounded between 0 and 1 and as a result, the numbers could be too small and will go towards 0."
    },
    {
        "term": "Quiz type: checkbox, Which of the following are useful for applications for parts of speech tagging? | Named Entity Recognition | Sentiment Analysis | Speech recognition | Coreference Resolution",
        "definition": "Named Entity Recognition|Speech recognition|Coreference Resolution"
    }, {
        "term": "Quiz type: radio, Corpus: ‚ÄúIn every place of great resort the monster was the fashion. They sang of it in the cafes, ridiculed it in the papers, and represented it on the stage. ‚Äù (Jules Verne, Twenty Thousand Leagues under the Sea) In the context of our corpus, what is the probability of word ‚Äúpapers‚Äù following the phrase ‚Äúit in the‚Äù. | P(papers|it in the) = 0 | P(papers|it in the) =1 | P(papers|it in the) = 2/3 | P(papers|it in the) = 1/2",
        "definition": "P(papers|it in the) = 1/2"
    },
    {
        "term": "Quiz type: radio, Given these conditional probabilities P(Mary)=0.1; P(likes)=0.2; P(cats)=0.3 . P(Mary|likes) =0.2; P(likes|Mary) =0.3; P(cats|likes)=0.1; P(likes|cats)=0.4 Approximate the probability of the following sentence with bigrams: ‚ÄúMary likes cats‚Äù | P(Mary likes cats) = 0.003 | P(Mary likes cats) = 0 | P(Mary likes cats) = 0.008 | P(Mary likes cats) =1",
        "definition": "P(Mary likes cats) = 0.003"
    },
    {
        "term": "Quiz type: radio, Given these conditional probabilities P(Mary)=0.1; P(likes)=0.2; P(cats)=0.3 P(Mary|<s>)=0.2; P(</s>|cats)=0.6 P(likes|Mary) =0.3; P(cats|likes)=0.1 Approximate the probability of the following sentence with bigrams: ‚Äú<s> Mary likes cats </s>‚Äù | P(<s> Mary likes cats </s>) = 1 | P(<s> Mary likes cats </s>) = 0 | P(<s> Mary likes cats </s>) = 0.0036 | P(<s> Mary likes cats </s>) = 0.003",
        "definition": "P(<s> Mary likes cats </s>) = 0.0036"
    },
    {
        "term": "Quiz type: radio, Given the logarithm of these conditional probabilities: log(P(Mary|<s>))=-2; log(P(</s>|cats))=-1 log(P(likes|Mary)) =-10; log(P(cats|likes))=-100 Approximate the log probability of the following sentence with bigrams : ‚Äú<s> Mary likes cats </s>‚Äù | log(P(<s> Mary likes cats </s>)) = -112 | log(P(<s> Mary likes cats </s>)) = 2000 | log(P(<s> Mary likes cats </s>)) = 113 | log(P(<s> Mary likes cats </s>)) = -113",
        "definition": "log(P(<s> Mary likes cats </s>)) = -113"
    },
    {
        "term": "Quiz type: radio, Given the logarithm of these conditional probabilities: log(P(Mary|<s>))=-2; log(P(</s>|cats))=-1 log(P(likes|Mary)) =-10; log(P(cats|likes))=-100 Assuming our test set is W=‚Äú<s> Mary likes cats </s>‚Äù, what is the model‚Äôs perplexity. | log PP(W) = -113 | log PP(W) = (-1/5)*(-113) | log PP(W) = (-1/4)*(-113) | log PP(W) = (-1/5)*113",
        "definition": "log PP(W) = (-1/4)*(-113)"
    },
    {
        "term": "Quiz type: radio, Given the training corpus and minimum word frequency=2, how would the vocabulary for corpus preprocessed with <UNK> look like? ‚Äú<s> I am happy I am learning </s> <s> I am happy I can study </s>‚Äù | V = (I,am,happy,I,am) | V = (I,am,happy,learning,can,study,<UNK>) | V = (I,am,happy,learning,can,study) | V = (I,am,happy)",
        "definition": "V = (I,am,happy)"
    },
    {
        "term": "Quiz type: radio, Corpus: ‚ÄúI am happy I am learning‚Äù In the context of our corpus, what is the estimated probability of word ‚Äúcan‚Äù following the word ‚ÄúI‚Äù using the bigram model and add-k-smoothing where k=3. | P(can|I) = 0 | P(can|I) =1 | P(can|I) = 3/(2+3*4) | P(can|I) = 3/(3*4)",
        "definition": "P(can|I) = 3/(2+3*4)"
    },
    {
        "term": "Quiz type: checkbox, Which of the following are applications of n-gram language models? | Speech recognitions | Auto-complete | Auto-correct | Augmentative communication | Sentiment Analysis",
        "definition": "Speech recognitions|Auto-complete|Auto-correct|Augmentative communication"
    },
    {
        "term": "Quiz type: radio, The higher the perplexity score the more our corpus will make sense. | True | False",
        "definition": "False"
    },
    {
        "term": "Quiz type: radio, The perplexity score increases as we increase the number of <UNK> tokens. | True. | False.",
        "definition": "False."
    }, {
        "term": "Quiz type: radio, Which one of the following word representations is most likely to correspond to a word embedding representation in a general-purpose vocabulary? In other words, which one is most likely to capture meaning and important information about the words? | car -> 2\n\ncaravan -> 3 | car -> (0.1 1)\n\ncaravan -> (-0.1 0.9) | car -> (0 1 0 0)\n\ncaravan -> (0 0 1 0) | car -> (1 0.1)\n\ncaravan -> (-1 -0.9)",
        "definition": "car -> (0.1 1)\n\ncaravan -> (-0.1 0.9)"
    },
    {
        "term": "Quiz type: radio, Which one of the following statements is correct? | To learn word embeddings you only need a vocabulary and an embedding method. | Learning word embeddings using a machine learning model is unsupervised learning as the input data set is not labelled. | The objective of a machine learning model that learns word embeddings is to predict word embeddings. | The meaning of the words, as carried by the word embeddings, depends on the embedding approach.",
        "definition": "The meaning of the words, as carried by the word embeddings, depends on the embedding approach."
    },
    {
        "term": "Quiz type: radio, Which one of the following statements is false? | You need to train a deep neural network to learn word embeddings. | word2vec-based models cannot create word embeddings for words they did not see in the corpus they were trained on. | ELMo may have different word embeddings for the word \"stable\" depending on the context. | You can use a pre-trained BERT model to learn word embeddings on a previously unseen corpus.",
        "definition": "You need to train a deep neural network to learn word embeddings."
    },
    {
        "term": "Quiz type: radio, Consider the corpus \"A robot may not injure a human being or, through inaction, allow a human being to come to harm.\" and assume you are preparing data to train a CBOW model. Ignoring punctuation, for a context half-size of 3, what are the context words of the center word \"inaction\"? | ‚Äúbeing inaction human‚Äù | ‚Äúbeing or through allow a human‚Äù | ‚Äúbeing or through inaction allow a human‚Äù | ‚Äúthrough inaction allow‚Äù",
        "definition": "‚Äúbeing or through allow a human‚Äù"
    },
    {
        "term": "Quiz type: radio, Which one of the following statements is false? | Given the corpus \"I think therefore I am\", the word \"think\" could be represented by the one-hot vector (1 0 0 0). | Consider the corpus \"A robot may not injure a human being or, through inaction, allow a human being to come to harm.\" and assume you are preparing data to train a CBOW model. Ignoring punctuation, for a context size of 3, the context words of the center word \"inaction\" are: \"a\", \"allow\", \"being\", \"human\", \"or\", and \"through\" | The continuous bag-of-words model learns to predict context words given a center word. | Given the corpus \"I think therefore I am\", the word \"you\" cannot be represented.",
        "definition": "The continuous bag-of-words model learns to predict context words given a center word."
    },
    {
        "term": "Quiz type: radio, You are designing a neural network for a CBOW model that will be trained on a corpus with a vocabulary of 8000 words. If you want it to learn 400-dimensional word embedding vectors, what should be the sizes of the input, hidden, and output layers? | 8000 (input layer), 8000 (hidden layer), 400 (output layer) | 400 (input layer), 400 (hidden layer), 8000 (output layer) | 8000 (input layer), 400 (hidden layer), 8000 (output layer) | 8000 (input layer), 400 (hidden layer), 400 (output layer)",
        "definition": "8000 (input layer), 400 (hidden layer), 8000 (output layer)"
    },
    {
        "term": "Quiz type: radio, If you are designing a neural network for a CBOW model that will be trained on a corpus of 8000 words, and if you want it to learn 400-dimensional word embedding vectors, what should be the size of W1, the weighting matrix between the input layer and hidden layer, if it is fed training examples in batches of 16 examples represented by a 8000 row by 16 column matrix? Hint: if X is the input matrix, H the matrix for the hidden layer, and B1 the bias matrix, then H = ReLU(W1X + B1). | 400 rows by 16 columns | 400 rows by 8000 columns | 16 rows by 8000 columns | 8000 rows by 16 columns",
        "definition": "400 rows by 8000 columns"
    },
    {
        "term": "Quiz type: radio, Given the input vector x below, a trained continuous bag-of-words model outputs the vector ≈∑ below. What is the word predicted by the model? | I | Think | Therefore | am",
        "definition": "Therefore"
    },
    {
        "term": "Quiz type: radio, The following weighting matrix W_1 has been learned after training a CBOW model. You are also given word-to-row mapping for the input column vectors. What is the word embedding vector for \"ring\"? | [-1.9; -1.18;¬† 2.61; -1.7; -4.36; -4.54] | [-2.39; -1.3; -1.7; 1.75] | [4.56; -2.94; 2.61; -1.16] | [0; 0; 1; 0; 0; 0]",
        "definition": "[4.56; -2.94; 2.61; -1.16]"
    },
    {
        "term": "Quiz type: checkbox, Select all that are correct. | You can perform intrinsic evaluation by using a clustering algorithm to group similar word embedding vectors, and determining if the clusters capture related words.  | To evaluate word embeddings with intrinsic evaluation, you use the word embeddings to perform an external task, which is typically the real-world task that you initially needed the word embeddings for. Then, use the performance metric of this task as a proxy for the quality of the word embeddings. | Extrinsic evaluation evaluates actual usefulness of embeddings, is time consuming and is more difficult to trouble shoot.  | To evaluate word embeddings with extrinsic evaluation, you use the word embeddings to perform an external task, which is typically the real-world task that you initially needed the word embeddings for. Then, use the performance metric of this task as a proxy for the quality of the word embeddings.",
        "definition": "You can perform intrinsic evaluation by using a clustering algorithm to group similar word embedding vectors, and determining if the clusters capture related words. |Extrinsic evaluation evaluates actual usefulness of embeddings, is time consuming and is more difficult to trouble shoot. |To evaluate word embeddings with extrinsic evaluation, you use the word embeddings to perform an external task, which is typically the real-world task that you initially needed the word embeddings for. Then, use the performance metric of this task as a proxy for the quality of the word embeddings."
    }, {
        "term": "Quiz type: radio, For the embedding layer in your model, you‚Äôd have to learn a matrix of weights of what size? | Equal to your vocabulary times the dimension of the number of classes | Equal to the dimension of the embedding times the first dimension of the matrix in the first layer. | Equal to your vocabulary times the dimension of the embedding | Equal to your vocabulary times the dimension of the number of layers",
        "definition": "Equal to your vocabulary times the dimension of the embedding"
    },
    {
        "term": "Quiz type: radio, What would be the probability of a five word sequence using a penta-gram? | a | b | c | d",
        "definition": "b"
    },
    {
        "term": "Quiz type: radio, The number of parameters in an RNN is the same regardless of the input's length. | False | True.",
        "definition": "True."
    },
    {
        "term": "Quiz type: checkbox, Select all the examples that correspond to a ‚Äúmany to one‚Äù architecture. | An RNN which inputs a sentiment and generates a sentence. | An RNN which inputs a sentence and determines the sentiment. | An RNN which inputs a topic and generates a conversation about that topic. | An RNN which inputs a conversation and determines the topic.",
        "definition": "An RNN which inputs a sentence and determines the sentiment.|An RNN which inputs a conversation and determines the topic."
    },
    {
        "term": "Quiz type: radio, What should be the size of matrix , if had size 4x1 and 10x1? | 4x14 | 14x4 | 4x4 | 14x14",
        "definition": "4x14"
    },
    {
        "term": "Quiz type: radio, In the next equation, why is there a division by the number of time steps but not one for the number of classification categories? | Because there is just one value in every vector different from zero. | Because the equation is wrong. | Because this equation is given for a single example. | Because for most classification tasks there are only two categories.",
        "definition": "Because this equation is given for a single example."
    },
    {
        "term": "Quiz type: radio, What problem, related to vanilla RNNs, do GRUs tackle? | Loss of relevant information for long sequences of words. | Overfitting | High computational time for training and prediction. | Restricted flow of information from the past to the present.",
        "definition": "Loss of relevant information for long sequences of words."
    },
    {
        "term": "Quiz type: radio, Bidirectional RNNs are acyclic graphs, which means that the computations in one direction are independent from the ones in the other direction. | True | False",
        "definition": "True"
    },
    {
        "term": "Quiz type: checkbox, Compared to Traditional Language models which of the following problems does an RNN help us with? | They are much simpler to understand. | They require almost no knowledge to use when compared to the traditional n-gram model. | Helps us solve memory issues. | Helps us solve RAM issues.",
        "definition": "Helps us solve memory issues.|Helps us solve RAM issues."
    },
    {
        "term": "Quiz type: radio, What type of RNN structure would you use when implementing machine translation? | One to many | Many to one | Many to Many | One to one",
        "definition": "Many to Many"
    }, {
        "term": "In the scan() function the variable cur_value corresponds to the hidden state in an RNN. A. True B. False",
        "definition": "A"
    }, {
        "term": "Identify the correct order of the gates that information flows through in an LSTM unit. A. Input gate, forget gate, output gate. B. Forget gate, input gate, output gate. C. Output gate, forget gate, input gate. D. Forget gate, output gate, input gate",
        "definition": "B"
    }, {
        "term": "Which are some applications of LSTMs? A. Music composition B. Image captioning C. Next character prediction D. Chatbots E. Speech recognition",
        "definition": "ABCDE"
    }, {
        "term": "The tanh layer ensures the values in your network stay numerically stable, by squeezing all values between -1 and 1. This prevents any of the values from the current inputs from becoming so large that they make the other values insignificant. A. False B. True",
        "definition": "B"
    }, {
        "term": "What type of architecture is a named entity recognition using? A. Many to many B. Many to one C. One to many",
        "definition": "A"
    }, {
        "term": "Extract the named entities from the following sentence: Younes, a Moroccan artificial intelligence engineer, travelled to France for a conference. A. Younes, Moroccan, engineer. B. Younes, Moroccan, France. C. Younes, Moroccan, conference. D. Younes, Moroccan engineer, France.",
        "definition": "B"
    }, {
        "term": "In a vectorized representation of your data, equal sequence length allows more efficient batch processing. A. False B. True.",
        "definition": "B"
    }, {
        "term": "Which built-in Python method would you use to iterate over your test set during the evaluation step? Assuming you are using a data generator. A. next() B. slice() C. list() D. enumerate()",
        "definition": "A"
    }, {
        "term": "Why is it important to mask padded tokens when computing the loss? A. We add the loss of the padded tokens independently. B. Padded tokens are not part of the data and are just used to help us keep the same sequence length for more efficient batch processing. We should not include their loss.",
        "definition": "B"
    }, {
        "term": "In which of the following orders should we train an Named Entity Recognition with an LSTM? A. Create a tensor for each input and its corresponding number Put them in a batch => 64, 128, 256, 512 ... Run the output through a dense layer Feed it into an LSTM unit Predict using a log softmax over K classes B. Create a tensor for each input and its corresponding number Put them in a batch => 64, 128, 256, 512 ... Run the output through a dense layer Predict using a log softmax over K classes Feed it into an LSTM unit C. Create a tensor for each input and its corresponding number Put them in a batch => 64, 128, 256, 512 ... Feed it into an LSTM unit Run the output through a dense layer Predict using a log softmax over K classes",
        "definition": "C"
    }, {
        "term": "LSTMS solve vanishing/exploding gradient problems when compared to basic RNNs. A. True B. False",
        "definition": "A"
    }, {
        "term": "Classification allows you to identify similarity between two things while siamese networks allow you to categorize things. A. True B. False",
        "definition": "B"
    }, {
        "term": "Do the two subnetworks in a siamese network share the same parameters? A. N‚Äão B. Y‚Äães",
        "definition": "B"
    }, {
        "term": "When training a siamese network to identify duplicates, which pairs of questions from the following questions do you expect to have the highest cosine similarity ? Is learning NLP useful for me to get a job? (ANCHOR) What should I learn to get a job? (POSITIVE) Where is the job? (NEGATIVE) A. Anchor, Positive B. Anchor, Negative C. Negative, Positive",
        "definition": "A"
    }, {
        "term": "In the triplet loss function below, will decreasing the hyperparameter alpha from 0.5 to 0.2 require more, or less, optimization during training ? diff = s(A,N) ‚àí s(A,P) L(A,P,N) = max(diff+Œ±, 0) A. Less B. More.",
        "definition": "A"
    }, {
        "term": "The orange square below corresponds to the similarity score of question duplicates? A. True B. False",
        "definition": "B"
    }, {
        "term": "What is the closest negative in this set of numbers assuming a duplicate pair similarity of 0.6? [-0.9,-0.4,0.4, 0.8] A. -0.9 B. -0.4 C. 0.4 D. 0.8",
        "definition": "C"
    }, {
        "term": "In one shot learning, is any retraining required when new classes are added? For example, a new bank customer's signature. A. Y‚Äães B. N‚Äão",
        "definition": "B"
    }]
}